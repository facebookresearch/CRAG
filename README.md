# CRAG: Comprehensive RAG Benchmark

The Comprehensive RAG Benchmark (CRAG) is a rich and comprehensive factual question answering benchmark designed to advance research in RAG. Besides question-answer pairs, CRAG provides mock APIs to simulate web and knowledge graph search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds.

This repository is migrated from [meta-comprehensive-rag-benchmark-kdd-cup-2024](https://gitlab.aicrowd.com/aicrowd/challenges/meta-comprehensive-rag-benchmark-kdd-cup-2024).

## üß™ Updates

To compare popular commercial Russian LLMs we've made several updates in original CRAG repo code. 

Models added: 
- YandexGPT PRO (API wrapper): `./models/vanilla_yandex_gpt_baseline.py`
- Sber GigaCHAT (in progress)

For testing purposes and to follow the structure of our experiments we've created utils functions 
to filter and/or limit the existing dataset: 
`./utils/`:
- `filter_data.py` - create filtered dataset based on question type (Task 1 & 2)
- `balance_data.py` - create balanced uniform dataset with all question types (Task 1 & 2)
- `compress_data.py` - compress created dataset into .bz2 format 

## üìä Dataset and Mock APIs

Please find more details about the CRAG dataset (download, schema, etc.) in [docs/dataset.md](docs/dataset.md) and mock APIs in [mock_api](mock_api).


## üìè Evaluation Metrics
RAG systems are evaluated using a scoring method that measures response quality to questions in the evaluation set. Responses are rated as perfect, acceptable, missing, or incorrect:

- Perfect: The response correctly answers the user question and contains no hallucinated content.

- Acceptable: The response provides a useful answer to the user question, but may contain minor errors that do not harm the usefulness of the answer.

- Missing: The answer does not provide the requested information. Such as ‚ÄúI don‚Äôt know‚Äù, ‚ÄúI‚Äôm sorry I can‚Äôt find ‚Ä¶‚Äù or similar sentences without providing a concrete answer to the question.

- Incorrect: The response provides wrong or irrelevant information to answer the user question


Auto-evaluation: 
- Automatic evaluation employs rule-based matching and LLM assessment to check answer correctness. It will assign three scores: correct (1 point), missing (0 points), and incorrect (-1 point).


Please refer to [local_evaluation.py](local_evaluation.py) for more details on how the evaluation was implemented.

## ‚úçÔ∏è How to run end-to-end evaluation?
1. **Install** specific dependencies
    ```bash
    uv pip install -r requirements.txt
    ```

2. To use YandexGPT and Gigachat models:
    - get credentials to access APIs from corresponding provider:
        - [YandexGPT API](https://yandex.cloud/en/docs/foundation-models/quickstart/yandexgpt)
        - [GigaCHAT API](https://developers.sber.ru/docs/ru/gigachat/api/overview)
        - [OpenAI API](https://openai.com/index/openai-api/)
    - create `.env` file in the root of the project;
    - provide api keys following the example from `.env_example`;
    - set names of models you want to use

2. Please follow the instructions in [models/README.md](models/README.md) for instructions and examples on how to write your own models.

3. After writing your own model(s), update [models/user_config.py](models/user_config.py)

   For example, in models/user_config.py, specify InstructModel to call llama3-8b-instruct model
   ```bash
   from models.vanilla_llama_baseline import InstructModel 
   UserModel = InstructModel

   ```

4. Test your model locally using `python local_evaluation.py`. This script will run answer generation and auto-evaluation.


## üèÅ Baselines
We include three baselines for demonstration purposes, and you can read more about them in [docs/baselines.md](docs/baselines.md).


## Citations

```
@article{yang2024crag,
      title={CRAG -- Comprehensive RAG Benchmark}, 
      author={Xiao Yang and Kai Sun and Hao Xin and Yushi Sun and Nikita Bhalla and Xiangsen Chen and Sajal Choudhary and Rongze Daniel Gui and Ziran Will Jiang and Ziyu Jiang and Lingkun Kong and Brian Moran and Jiaqi Wang and Yifan Ethan Xu and An Yan and Chenyu Yang and Eting Yuan and Hanwen Zha and Nan Tang and Lei Chen and Nicolas Scheffer and Yue Liu and Nirav Shah and Rakesh Wanga and Anuj Kumar and Wen-tau Yih and Xin Luna Dong},
      year={2024},
      journal={arXiv preprint arXiv:2406.04744},
      url={https://arxiv.org/abs/2406.04744}
}
```

## License

This project is licensed under the [Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0)](LICENSE). This license permits sharing and adapting the work, provided it's not used for commercial purposes and appropriate credit is given. For a quick overview, visit [Creative Commons License](https://creativecommons.org/licenses/by-nc/4.0/).
